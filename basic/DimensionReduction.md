<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 数据降维

## 背景介绍

- 机器学习中，数据被表示为向量；为了处理高维向量，需要消耗大量的计算资源

- 向量不同维度之间存在某种关系，增加了问题分析的复杂性，也导致了数据冗余

- 降维的目的就是在降低数据维度的同时，尽量减少关键信息的损失

## PCA

- Principal Component Analysis，即主成分分析

### 基本思想

- 假设原始空间维度为 \\(N\\)，降维后的空间维度为 \\(D\\)，其中 \\(D \leq N\\)

- 计算最佳投影矩阵，把 \\(N\\) 维空间中的点投影到 \\(D\\) 维空间，优化目标如下：

	- 同一维度的方差尽量大，保证投影后的点尽量分散

	- 不同维度的协方差尽量小，消除不同维度之间的依赖

### 推导过程

- 把 \\(N\\) 维向量 \\(x\_{i}\\) 投影到 \\(D\\) 维空间，相当于在 \\(N\\) 维空间中选择 \\(D\\) 个基向量，将 \\(x\_{i}\\) 在 \\(D\\) 个基向量上的投影作为 \\(D\\) 维空间中的坐标

- 矩阵化表示为：

	$$ \\left[ \\begin{matrix} y\_{1} & y\_{2} & ... & y\_{M} \\end{matrix} \\right] = \\left[ \\begin{matrix} p\_{1} \\\\ p\_{2} \\\\ ... \\\\ p\_{D} \\end{matrix} \\right] \\left[ \\begin{matrix} x\_{1} & x\_{2} & ... & x\_{M} \\end{matrix} \\right] $$

	- 其中 \\(M\\) 是待投影向量数，\\(y\_{i}\\) 是投影后的 \\(D\\) 维列向量，\\(p\_{i}\\) 是 \\(N\\) 维空间中投影基向量的行向量表示，\\(x\_{i}\\) 是投影前的 \\(N\\) 维列向量

	- 上式简记为：

		$$ Y = PX $$

- 假设 \\(X\\) 的协方差矩阵为 \\(C\\)，以 \\(N=2\\) 为例：

	- \\(X\\) 可表示为：

		$$ X = \\left[ \\begin{matrix} a\_{1} & a\_{2} & ... & a\_{M} \\\\ b\_{1} & b\_{2} & ... & b\_{M} \end{matrix} \\right] $$

	- 协方差矩阵 \\(C\\) 计算如下：

		$$ C = \frac{1}{M} \\left[ \\begin{matrix} \sum\_{i=1}^{M} (a\_{i}-\mu\_{a})^{2} & \sum\_{i=1}^{M} (a\_{i}-\mu\_{a}) \cdot (b\_{i}-\mu\_{b}) \\\\ \sum\_{i=1}^{M} (b\_{i}-\mu\_{b}) \cdot (a\_{i} - \mu\_{a}) & \sum\_{i=1}^{M} (b\_{i}-\mu\_{b})^{2} \\end{matrix} \\right] $$

	- 如果对 \\(X\\) 提前进行中心化处理，即每一维度都减掉均值可得：

		$$ C = \frac{1}{M} XX^{T} $$

- 而 \\(Y\\) 的协方差矩阵 \\(D\\) 计算如下：

	$$ D = \frac{1}{M} YY^{T} = \frac{1}{M} (PX)(PX)^{T} = P \left( \frac{1}{M} XX^{T} \right) P^{T} = PCP^{T} $$

- 优化目标是最大化 \\(D\\) 的对角线元素，并使非对角元素为 \\(0\\)

	- 由于 \\(C\\) 是实对称矩阵，由实对称矩阵对角化可得：

		$$ \Lambda = Q^{T}CQ $$

		- 其中 \\(\Lambda\\) 为 \\(C\\) 特征值组成的对角矩阵，\\(Q\\) 的每一列是 \\(C\\) 单位正交化的特征向量

		- 关于实对称矩阵对角化，参考 [MatrixBasis.md](MatrixBasis.md)

	- 将 \\(C\\) 的 \\(N\\) 个特征值从大到小排列后，如果只保留 \\(Q\\) 前 \\(D\\) 列特征向量，此时 \\(\Lambda\\) 为 \\(C\\) 的前 \\(D\\) 个特征值组成的对角矩阵

	- 由于特征值表示对应特征向量方向上特征的重要程度，如果令 \\(P\\) 等于 \\(Q^{T}\\) 的前 \\(D\\) 行，能实现 PCA 的优化目标

- 即取 \\(XX^{T}\\) 的前 \\(D\\) 个特征向量组成 \\(P\\) 的每一行，对 \\(X\\) 投影得到 \\(Y=PX\\)

### 计算过程

- 生成原始数据矩阵 \\(X\\)

- 对 \\(X\\) 的每一维进行归一化处理

	- 首先进行中心化处理，减去每一维的均值；由推导过程决定

	- 然后进行方差单位化，除以每一维的标注差；消除不同维度间的数值影响

- 求解 \\(XX^{T}\\) 的前 \\(D\\) 个特征向量

	- 直接求解 \\(C=XX^{T}\\) 的特征向量

		$$ C x = \lambda x $$

		- 关于特征值求解，参考 [MatrixBasis.md](MatrixBasis.md)

	- 对 \\(X\\) 进行奇异值分解

		$$ X = U \Sigma V^{T} $$

		- \\(U\\) 的每一列都是 \\(XX^{T}\\) 的特征向量

		- 关于奇异值分解，参考 [MatrixDecomposition.md](MatrixDecomposition.md)

## LDA

- Linear Discriminant Analysis，即线性判别分析

### 基本思想

- 假设原始空间维度为 \\(N\\)，降维后的空间维度为 \\(D\\)，其中 \\(D \leq N\\)

- ，其中 \\(D \leq N\\)

- 计算投影矩阵 \\(W\\)，把 \\(N\\) 维空间中样本投影到 \\(D\\) 维空间，优化目标如下：

	- 让不同类尽量分开，即最大化不同类别间的距离

	- 让同一类尽量聚拢，即最小化同一类别内的距离

### 推导过程

- 以 \\(N\\) 维空间投影到 \\(1\\) 维空间的二分类为例：

	- \\(W=w\\) 是 \\(N\\) 维列向量

	- 投影点在同一直线上，结果 \\(y\\) 为标量：

		$$ y=w^{T}x $$

	- 设 \\(X\_{0}, \ X\_{1}\\) 表示两类样本的集合，\\(\mu\_{0}, \ \mu\_{1}\\) 表示两类样本在原始空间的均值向量，\\(\Sigma\_{0}, \ \Sigma\_{1}\\) 表示两类样本在原始空间的协方差矩阵，\\(\sigma\_{0}, \ \sigma\_{1}\\) 表示两类样本在投影空间的方差，计算如下：

	$$ \sigma\_{i} = \frac{1}{|X\_{i}|} \sum\_{x \in X\_{i}} (w^{T}x - w^{T}\mu\_{i}) ^ {2} = w^{T} \left( \frac{1}{|X\_{i}|} \sum\_{x \in X\_{i}} (x-\mu\_{i})(x-\mu\_{i})^{T} \right) w = w^{T} \Sigma\_{i} w $$

- 定义距离与散度矩阵如下：

	- 类间距离：

		$$ ||w^{T}\mu\_{0} - w^{T}\mu\_{1}||^{2} = w^{T} (\mu\_{0}-\mu\_{1})(\mu\_{0}-\mu\_{1})^{T} w $$

	- 类内距离：

		$$ \sigma\_{0} + \sigma\_{1} = w^{T} (\Sigma\_{0}+\Sigma\_{1}) w $$

	- 类间散度矩阵：

		$$ S\_{b} = (\mu\_{0}-\mu\_{1})(\mu\_{0}-\mu\_{1})^{T} $$

	- 类内散度矩阵：

		$$ S\_{w} = \Sigma\_{0} + \Sigma\_{1} $$

- 原问题相当于最大化：

	$$ J(w) = \frac{w^{T} S\_{b} w}{w^{T} S\_{w} w} $$

	- 由于分子、分母都是 \\(w\\) 的二次型，\\(J(w)\\) 的解与 \\(w\\) 长度无关，只与方向有关；由于 \\(w^{T} S\_{w} w\\) 为标量，对 \\(w\\) 进行如下约束：

		$$ w^{T} S\_{w} w = 1 $$

	- 上述问题转换为：

		$$ \min{\ -w^{T} S\_{b} w} \quad s.t. \ w^{T} S\_{w} w = 1 $$

- 由拉格朗日乘子法：

	$$ L(w, \lambda) = -w^{T} S\_{b} w + \lambda (w^{T} S\_{w} w - 1) $$

	- 对 \\(w\\) 求导并令导数为 \\(0\\) 可得：

		$$ S\_{b} w = \lambda S\_{w} w $$

- 由 \\(S\_{b}\\) 定义可知：

	$$ S\_{b} w = (\mu\_{0}-\mu\_{1}) \left( (\mu\_{0}-\mu\_{1})^{T} w \right) = (\mu\_{0}-\mu\_{1}) \cdot \lambda_{w} $$

	- 代入上式可得：

		$$ w = \frac{\lambda\_{w}}{\lambda} S\_{w}^{-1} (\mu\_{0} - \mu\_{1}) $$

	- 由于 \\(w\\) 的长度不影响最终结果，进一步可得：

		$$ w = S\_{w}^{-1} (\mu\_{0} - \mu\_{1}) $$

### 计算过程

- 计算均值向量 \\(\mu\_{0}, \ \mu\_{1}\\)

- 计算协方差矩阵 \\(\Sigma\_{0}, \ \Sigma\_{1}\\)

- 计算类内散度矩阵的逆 \\(S\_{w}^{-1} = (\Sigma\_{0} + \Sigma\_{1})^{-1}\\)

- 计算投影向量 \\(w = S\_{w}^{-1} (\mu\_{0} - \mu\_{1})\\)