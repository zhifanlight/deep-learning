<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 数据降维

## 背景介绍

- 机器学习中，数据被表示为向量；为了处理高维向量，需要消耗大量的计算资源

- 向量不同维度之间存在某种关系，增加了问题分析的复杂性，也导致了数据冗余

- 降维的目的就是在降低数据维度的同时，尽量减少关键信息的损失

## PCA

- 主成分分析

### 基本思想

- 假设原始空间维度为 \\(N\\)，降维后的空间维度为 \\(D\\)，其中 \\(D \leq N\\)

- 计算最佳投影矩阵，把 \\(N\\) 维空间中的点投影到 \\(D\\) 维空间，优化目标如下：

	- 同一维度的方差尽量大，保证投影后的点尽量分散

	- 不同维度的协方差尽量小，消除不同维度之间的依赖

### 推导过程

- 把 \\(N\\) 维向量 \\(x\_{i}\\) 投影到 \\(D\\) 维空间，相当于在 \\(N\\) 维空间中选择 \\(D\\) 个基向量，将 \\(x\_{i}\\) 在 \\(D\\) 个基向量上的投影作为 \\(D\\) 维空间中的坐标

- 矩阵化表示为：

	$$ \\left[ \\begin{matrix} y\_{1} & y\_{2} & ... & y\_{M} \\end{matrix} \\right] = \\left[ \\begin{matrix} p\_{1} \\\\ p\_{2} \\\\ ... \\\\ p\_{D} \\end{matrix} \\right] \\left[ \\begin{matrix} x\_{1} & x\_{2} & ... & x\_{M} \\end{matrix} \\right] $$

	- 其中 \\(M\\) 是待投影向量数，\\(y\_{i}\\) 是投影后的 \\(D\\) 维列向量，\\(p\_{i}\\) 是 \\(N\\) 维空间中投影基向量的行向量表示，\\(x\_{i}\\) 是投影前的 \\(N\\) 维列向量

	- 上式简记为：

		$$ Y = PX $$

- 假设 \\(X\\) 的协方差矩阵为 \\(C\\)，以 \\(N=2\\) 为例：

	- \\(X\\) 可表示为：

		$$ X = \\left[ \\begin{matrix} a\_{1} & a\_{2} & ... & a\_{M} \\\\ b\_{1} & b\_{2} & ... & b\_{M} \end{matrix} \\right] $$

	- 协方差矩阵 \\(C\\) 计算如下：

		$$ C = \frac{1}{M} \\left[ \\begin{matrix} \sum\_{i=1}^{M} (a\_{i}-\mu\_{a})^{2} & \sum\_{i=1}^{M} (a\_{i}-\mu\_{a}) \cdot (b\_{i}-\mu\_{b}) \\\\ \sum\_{i=1}^{M} (b\_{i}-\mu\_{b}) \cdot (a\_{i} - \mu\_{a}) & \sum\_{i=1}^{M} (b\_{i}-\mu\_{b})^{2} \\end{matrix} \\right] $$

	- 如果对 \\(X\\) 提前进行中心化处理，即每一维度都减掉均值可得：

		$$ C = \frac{1}{M} XX^{T} $$

- 而 \\(Y\\) 的协方差矩阵 \\(D\\) 计算如下：

	$$ D = \frac{1}{M} YY^{T} = \frac{1}{M} (PX)(PX)^{T} = P \left( \frac{1}{M} XX^{T} \right) P^{T} = PCP^{T} $$

- 优化目标是最大化 \\(D\\) 的对角线元素，并使非对角元素为 \\(0\\)

	- 由于 \\(C\\) 是实对称矩阵，由实对称矩阵对角化可得：

		$$ \Lambda = Q^{T}CQ $$

		- 其中 \\(\Lambda\\) 为 \\(C\\) 特征值组成的对角矩阵，\\(Q\\) 的每一列是 \\(C\\) 单位正交化的特征向量

		- 关于实对称矩阵对角化，参考 [MatrixBasis.md](MatrixBasis.md)

	- 将 \\(C\\) 的 \\(N\\) 个特征值从大到小排列后，如果只保留 \\(Q\\) 前 \\(D\\) 列特征向量，此时 \\(\Lambda\\) 为 \\(C\\) 的前 \\(D\\) 个特征值组成的对角矩阵

	- 由于特征值表示对应特征向量方向上特征的重要程度，如果令 \\(P\\) 等于 \\(Q^{T}\\) 的前 \\(D\\) 行，能实现 PCA 的优化目标

- 即取 \\(XX^{T}\\) 的前 \\(D\\) 个特征向量组成 \\(P\\) 的每一行，对 \\(X\\) 投影得到 \\(Y=PX\\)

### 计算过程

- 生成原始数据矩阵 \\(X\\)

- 对 \\(X\\) 的每一维进行归一化处理

	- 首先进行中心化处理，减去每一维的均值；由推导过程决定

	- 然后进行方差单位化，除以每一维的标注差；消除不同维度间的数值影响

- 求解 \\(XX^{T}\\) 的前 \\(D\\) 个特征向量

	- 直接求解 \\(C=XX^{T}\\) 的特征向量

		$$ C x = \lambda x $$

		- 关于特征值求解，参考 [MatrixBasis.md](MatrixBasis.md)

	- 对 \\(X\\) 进行奇异值分解

		$$ X = U \Sigma V^{T} $$

		- \\(U\\) 的每一列都是 \\(XX^{T}\\) 的特征向量

		- 关于奇异值分解，参考 [MatrixDecomposition.md](MatrixDecomposition.md)