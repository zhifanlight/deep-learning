<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# kNN

## 基本思想

- 分类算法的一种，属于有监督学习

- 如果某样本的 \\(k\\) 个最相似样本中的大多数都属于某个类别，那么该样本也很有可能属于该类别

## 算法流程

- 在特征空间中，寻找当前样本的前 \\(k\\) 个最相似样本

- 将这 \\(k\\) 个样本中出现次数最多的类别标签作为当前样本的类别标签

## 算法分析

- 没有明显的前期训练过程，将样本进行简单特征提取后，即可进行分类

- 对于给定实例 \\(x\\)，其最近邻的 \\(k\\) 个样本集合为 \\(N(x)\\)，覆盖 \\(N(x)\\) 的类别是 \\(c\_{j}\\)，那么误分类率是：

	$$
	\begin{align\*}
	\epsilon &= \frac{1}{k} \sum\_{x\_{i} \in N(x)} I(y\_{i} \neq c\_{j}) \newline
	&= \frac{1}{k} \sum\_{x\_{i} \in N(x)} \left( 1 - I(y\_{i} = c\_{j}) \right) \newline
	&= 1 - \frac{1}{k} \sum\_{x\_{i} \in N(x)} I(y\_{i} = c\_{j}) \newline
	\end{align\*}
	$$
	
	- 多数表决等价于最大化 \\(\sum\_{x\_{i} \in N(x)}\\)，等价于最小化经验风险

## KD 树创建

- 从数据的第一维开始，循环重复以下过程：

	- 寻找当前维度的中位数，并将当前数据集分为两个子集

	- 递归的在下一个维度上处理两个子集

- 时间复杂度：

	$$ T(n) = 2 \cdot T(\frac{n}{2}) + n \quad \Rightarrow \quad T(n) = O(nlogn) $$

## 维度灾难

- 当维度增大时，样本单位距离内的其他样本数量会减少，需要寻找更远的距离才能找到 \\(k\\) 个样本；但此时距离较远，找到的样本参考价值较小

- kNN 分类时，要计算当前样本与数据集中所有样本的距离，维度增加会导致大量计算