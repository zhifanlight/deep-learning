<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# 基本理论

## 统计学习三要素

### 模型

- 模型所要学习的条件概率分布、决策函数
    
- 模型的输入空间、输出空间

### 策略

- 经验风险最小化
    
	$$ \min\_{f \in F} \frac{1}{N} \sum\_{i=1}^{N} L(y\_{i}, \ f(x_{i})) $$
    
- 结构风险最小化
    
	$$ \min\_{f \in F} \frac{1}{N} \sum\_{i=1}^{N} L(y\_{i}, \ f(x\_{i})) + \lambda J(f) $$

### 算法

- 学习模型的具体计算方法

## 偏差—方差分解

- 以回归任务为例，假设样本为 \\(x\\)，真实标记为 \\(y\\)，在训练集中的标记为 \\(y\_{D}\\)，训练集 \\(D\\) 上学得模型的预测输出为 \\(f(x;D)\\)

- 学习算法的期望输预测：

	$$
	\bar{f}(x) = E\_{D} [f(x;D)]
	$$

- 使用不同训练集产生的方差：

	$$
	var(x) = E\_{D} \left[ (f(x;D) - \bar{f}(x))^{2} \right]
	$$
	
	- 方差衡量了数据集变动导致的学习性能的变化

- 由于标记问题产生的噪声：

	$$
	\epsilon^{2} = E\_{D} \left[ (y\_{D} - y)^{2} \right]
	$$
	
	- 噪声衡量了当前任务上任何模型所能达到的泛化误差下界

- 期望输出与真实标记的偏差：

	$$
	bias^{2}(x) = \left( \bar{f}(x) - y \right) ^{2}
	$$
	
	- 偏差衡量了模型本身的拟合能力

- 平均泛化误差：

	$$
	E(f;D) = E\_{D} \left[ (f(x;D) - y\_{D})^{2} \right]
	$$

- 误差的偏差—方差分解

	$$
	E(f;D) = var(x) + bias^{2}(x) + \epsilon^{2}
	$$
	
	- 在训练不足时，模型拟合能力不强，训练集变动不足以使模型拟合的能力发生显著变化，此时偏差主导了泛化误差

	- 在训练充足后，模型拟合能力较强，训练集的轻微变动都会导致模型发生显著变化，此时方差主导了泛化误差

## 监督 VS 无监督 VS 半监督

### 监督

- 使用有标签数据进行模型训练

### 无监督

- 通过无标记数据进行模型训练

### 半监督

- 同时使用有标签、无标签数据进行模型训练

## 生成模型 VS 判别模型

### 生成模型

- 通过学习联合概率分布 \\(P(X, Y)\\)，然后求出条件概率 \\(P(Y|X)\\) 作为预测模型：

	$$ P(Y|X) = \frac{P(X,Y)}{P(X)} $$

- 生成模型可以还原出联合分布 \\(P(X,Y)\\)

- 当存在隐变量时，仍然可以使用生成学习方法

- 典型的生成模型包括：朴素贝叶斯、高斯混合模型、隐马尔可夫模型

### 判别模型

- 直接学习决策函数 \\(f(X)\\) 或条件概率分布 \\(P(Y|X)\\) 作为预测模型

- 判别模型直接面对预测，往往准确率更高

- 典型的判别模型包括： kNN、决策树、逻辑回归、最大熵、SVM、神经网络

## 分类 VS 标注 VS 回归

### 分类

- 从数据集中学习的分类模型或分类决策函数，称为分类器

- 通过分类器对新的输入进行输出的预测，称为分类

### 标注

- 输入是一个观测序列，输出是一个标记序列或状态序列

- 分类问题的推广，分类的输出是标量，标注的输出是向量

### 回归

- 用于预测输入变量到输出变量的映射关系

- 选择一条函数曲线，使其很好地拟合已知数据并很好地预测未知数据