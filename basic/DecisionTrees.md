# 决策树

## 背景介绍

- 决策树是一个树结构：每个非叶节点表示一个特征的测试，每个分支代表这个特征的取值，每个叶节点代表一个类别

- 使用决策树进行决策时，从根节点开始，根据待分类的特征选择相应分支，直到到达叶节点，将叶节点的类别作为决策结果

- 决策树的生成只考虑局部最优，决策树的剪枝则考虑全局最优

## 决策树

### $\mathrm{ID3}$

#### 基本思想

- 计算当前子集中每个属性的信息增益，选择信息增益最大的属性进行分裂

  - 信息增益表示在给定条件下，信息不确定性的减少程度

#### 算法流程

- 初始化时，当前集合 $D$ 是整个训练集

- 计算当前集合 $D$ 的信息熵：

  $$
  \mathrm{Ent} \left( D \right) = - \sum_{k = 1}^{K} p_{k} \cdot \log_{2} \ {p_{k}}
  $$

  - 其中 $K$ 是 $D$ 中的类别数，$p_{k}$ 是每个类别的比例

- 计算当前集合 $D$ 中每个未处理属性 $a$ 的信息增益：

  $$
  \mathrm{Gain} \left( D, \ a \right) = \mathrm{Ent} \left( D \right) - \sum_{v \in V} \frac{|D_{v}|}{|D|} \mathrm{Ent} \left( D_{v} \right)
  $$

  - 其中 $V$ 是 $D$ 中 $a$ 属性的取值集合，$D_{v}$ 是 $D$ 中 $a = v$ 的样本集合

- 选择最大信息增益 $G_{\mathrm{max}}$ 对应的属性，将 $D$ 划分成不同子集 $D_{1}, \ D_{2}, \ \cdots, \ D_{|V|}$

- 重复上述三步，直到 $D_{i}$ 满足以下任一条件：

  - $D_{i}$ 中的样本已经属于同一类别，此时直接将该类别标签作为叶节点，停止划分

  - $G_{\mathrm{max}}$ 小于阈值 $\epsilon$，此时直接将 $D_{i}$ 所有样本划分为同一类别，并将大多数样本对应的类别标签作为叶节点，停止划分

  - 处理完所有属性后 $D_{i}$ 中的样本还不属于同一类别，此时将 $D_{i}$ 中大多数样本对应的类别标签作为叶节点，停止划分

#### 算法分析

- 每次分裂时，倾向于选择取值数目多的属性，而不是最好的属性

  - 极端情况下，比如计算 $\mathrm{ID}$ 属性对应的信息增益时，由于每个 $D_{v}$ 只有一个样本，因此每个 $\mathrm{Ent} \left( D_{v} \right) = 0$，此时信息增益最大，然而对分类无益

### $\mathrm{C4.5}$

#### 基本思想

- 计算当前子集中每个属性的信息增益、增益率，先找出信息增益高于平均值的属性，再从中选择增益率最高的属性进行分裂

  - 增益率越大，则信息增益越大，同时类别较少

#### 算法流程

- 与 $\mathrm{ID3}$ 基本相同，区别在于划分节点是最大增益率对应的属性

- 增益率计算如下：

  $$
  \mathrm{Ratio} \left( D, \ a \right) = \frac{\mathrm{Gain} \left( D, \ a \right)}{\mathrm{Split} \left( a \right)}
  $$

  - 其中 $\mathrm{Split} \left( a \right)$ 是属性 $a$ 的分裂熵：

    $$
    \mathrm{Split} \left( a \right) = - \sum_{v \in V} \frac{|D_{v}|}{|D|} \log_{2} \frac{|D_{v}|}{|D|}
    $$

    - 类别越多越混乱，分裂熵越大，增益率越小

#### 算法分析

- 解决了 $\mathrm{ID3}$ 选择分裂属性的问题

- 如果直接选择最大增益率对应的属性，倾向于选择取值数目少的属性

### $\mathrm{CART}$ 分类树

- $\mathrm{Classification \ and \ Regression \ Trees}$，即分类与回归树

#### 基本思想

- 计算当前子集中每个属性的基尼指数，选择基尼指数最小的属性进行分裂

  - 基尼指数衡量样本集的类别不纯净度，基尼指数越小，样本集类别越纯净

- 对于二分类问题，基尼指数、半经验熵的概率分布分别为：

  $$
  2p \left( 1 - p \right) \qquad -\frac{1}{2} \left( p \log p + \left( 1 - p \right) \log \left( 1 - p \right) \right)
  $$

  - 两条函数曲线逼近，因此基尼指数也可也衡量样本集合的不确定性

#### 算法流程

- 初始化时，当前集合 $D$ 是整个训练集

- 计算当前集合 $D$ 的基尼值：

  $$
  \mathrm{Gini} \left( D \right) = 1 - \sum_{k = 1}^{K} p_{k}^{2}
  $$

  - 其中 $K$ 是 $D$ 中的类别数，$p_{k}$ 是每个类别的比例

  - 当所有样本属于一类时，基尼值为 $0$；类别越多，基尼值越大

- 计算当前集合 $D$ 中每个未处理属性 $a$ 的基尼指数：

  $$
  \mathrm{Index} \left( D, \ a \right) = \frac{|D_{y}|}{|D|} \mathrm{Gini} \left( D_{y} \right) + \frac{|D_{n}|}{|D|} \mathrm{Gini} \left( D_{n} \right)
  $$

  - 其中 $D_{y}$ 是 $D$ 中 $a = v$ 的样本集合，$D_{n}$ 是 $D$ 中 $a \neq v$ 的样本集合

- 选择最小基尼指数 $I_{\mathrm{min}}$ 对应的属性，将 $D$ 划分成两个子集 $D_{1}, \ D_{2}$

- 重复上述三步，直到 $D_{i}$ 满足以下任一条件：

  - $D_{i}$ 中的样本已经属于同一类别，此时直接将该类别标签作为叶节点，停止划分

  - $I_{\mathrm{min}}$ 小于阈值 $\epsilon$，此时基本基本属于同一类，将大多数样本对应的类别标签作为叶节点，停止划分

  - $D_{i}$ 的样本数小于阈值 $T$，此时继续划分的意义不大，将大多数样本对应的类别标签作为叶节点，停止划分

#### 算法分析

- 分类效果较好

- 无论属性取值数目多少，每次仅把当前集合分为两个子集

### $\mathrm{CART}$ 回归树

- $\mathrm{Classification \ and \ Regression \ Trees}$，即分类与回归树

#### 基本思想

- 计算当前子集中每个属性、切分点下的平方误差，选择平方误差最小的属性及切分点进行划分

#### 算法流程

- 初始化时，当前集合 $D$ 是整个训练集

- 计算当前集合 $D$ 中每个未处理属性 $a$，切分点 $s$ 的平方误差：

  $$
  \mathrm{Error} \left( a, \ s \right) = \min_{a, \ s} \left[ \min_{c_{y}} \sum_{\left( x_{i}, \ y_{i} \right) \in D_{y}} \left( y_{i} - c_{y} \right)^{2} + \min_{c_{n}} \sum_{\left( x_{i}, \ y_{i} \right) \in D_{n}} \left( y_{i} - c_{n} \right)^{2} \right]
  $$

  - 其中 $D_{y}$ 是 $D$ 中 $a < v$ 的样本集合，$D_{n}$ 是 $D$ 中 $a > v$ 的样本集合，$c_{y}$ 是 $D_{y}$ 上的预测输出，$c_{n}$ 是 $D_{n}$ 上的预测输出

  - 求和项 $\sum$ 对 $c_{y}, \ c_{n}$ 求导并令导数为 $0$ 可得最优 $c_{y}, \ c_{n}$：

    $$
    c_{y} = \frac{1}{|D_{y}|} \sum_{\left( x_{i}, \ y_{i} \right) \in D_{y}}{y_{i}} \qquad \qquad c_{n} = \frac{1}{|D_{n}|} \sum_{\left( x_{i}, \ y_{i} \right) \in D_{n}} y_{i}
    $$

    - 即最优预测值是相应集合上的平均值

- 选择最小平方误差 $E_{\mathrm{min}}$ 对应的属性即切分点，将 $D$ 划分成两个子集 $D_{1}, \ D_{2}$

- 重复上述三步，直到 $D_{i}$ 满足以下任一条件：

  - $D_{i}$ 中所有样本的输出值相同，停止划分

  - 对 $D_{i}$ 划分后基本不能降低误差，停止划分

  - 对 $D_{i}$ 划分后的某个子集中样本极少，停止划分

#### 算法分析

- 寻找最佳属性及最佳切分点时，时间复杂度较高

### 决策树剪枝

- 为了防止过拟合，需要对决策树进行剪枝处理

#### 预剪枝

- 在生成决策树的过程中进行剪枝，可能会导致欠拟合

- 从根开始依次处理每一个节点，对于当前选中的待分裂属性：

  - 如果划分后可以提升泛化性能，则进行划分；否则直接把当前节点置为叶节点，类别标签为当前集合中样本数最多的类别

#### 后剪枝

- 先生成决策树再进行剪枝，时间开销较大

- 从底层每一个非叶节点开始：

  - 如果合并其子树可以提升泛化性能，则将该子树替换为叶节点，类别标签为当前集合中样本数最多的类别；否则保留子树，不进行合并

## 随机森林

### 基本思想

- 随机森林由多棵决策树组成，决策树之间无关联

- 在预测时，每棵决策树都进行预测（投票），得票最多的结果作为最终的分类

### 算法流程

- 假设训练集有 $N$ 个样本，每个样本有 $M$ 个特征

- 有放回地随机选择 $N$ 个样本，以减少过拟合

- 随机选择 $m$ 个特征（$m \ll M$），在选出的训练集上训练一棵决策树

  - 由于样本和特征的选择都具有随机性，训练时不用剪枝

- 统计所有决策树的预测结果，得票最多的结果作为最终的分类

### 算法分析

- 每棵树都之间无关联，可以进行并行训练

- 每棵树随机选择训练样本，不容易出现过拟合

- 每棵树随机处理部分特征，可以处理高维数据，而不用特征选择