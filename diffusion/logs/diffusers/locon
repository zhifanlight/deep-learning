    lora_te_text_model_encoder_layers_0_mlp_fc1 torch.Size([3072, 768])
    lora_te_text_model_encoder_layers_0_mlp_fc2 torch.Size([768, 3072])
    lora_te_text_model_encoder_layers_0_self_attn_k_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_0_self_attn_out_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_0_self_attn_q_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_0_self_attn_v_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_1_mlp_fc1 torch.Size([3072, 768])
    lora_te_text_model_encoder_layers_1_mlp_fc2 torch.Size([768, 3072])
    lora_te_text_model_encoder_layers_1_self_attn_k_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_1_self_attn_out_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_1_self_attn_q_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_1_self_attn_v_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_2_mlp_fc1 torch.Size([3072, 768])
    lora_te_text_model_encoder_layers_2_mlp_fc2 torch.Size([768, 3072])
    lora_te_text_model_encoder_layers_2_self_attn_k_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_2_self_attn_out_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_2_self_attn_q_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_2_self_attn_v_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_3_mlp_fc1 torch.Size([3072, 768])
    lora_te_text_model_encoder_layers_3_mlp_fc2 torch.Size([768, 3072])
    lora_te_text_model_encoder_layers_3_self_attn_k_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_3_self_attn_out_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_3_self_attn_q_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_3_self_attn_v_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_4_mlp_fc1 torch.Size([3072, 768])
    lora_te_text_model_encoder_layers_4_mlp_fc2 torch.Size([768, 3072])
    lora_te_text_model_encoder_layers_4_self_attn_k_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_4_self_attn_out_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_4_self_attn_q_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_4_self_attn_v_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_5_mlp_fc1 torch.Size([3072, 768])
    lora_te_text_model_encoder_layers_5_mlp_fc2 torch.Size([768, 3072])
    lora_te_text_model_encoder_layers_5_self_attn_k_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_5_self_attn_out_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_5_self_attn_q_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_5_self_attn_v_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_6_mlp_fc1 torch.Size([3072, 768])
    lora_te_text_model_encoder_layers_6_mlp_fc2 torch.Size([768, 3072])
    lora_te_text_model_encoder_layers_6_self_attn_k_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_6_self_attn_out_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_6_self_attn_q_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_6_self_attn_v_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_7_mlp_fc1 torch.Size([3072, 768])
    lora_te_text_model_encoder_layers_7_mlp_fc2 torch.Size([768, 3072])
    lora_te_text_model_encoder_layers_7_self_attn_k_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_7_self_attn_out_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_7_self_attn_q_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_7_self_attn_v_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_8_mlp_fc1 torch.Size([3072, 768])
    lora_te_text_model_encoder_layers_8_mlp_fc2 torch.Size([768, 3072])
    lora_te_text_model_encoder_layers_8_self_attn_k_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_8_self_attn_out_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_8_self_attn_q_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_8_self_attn_v_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_9_mlp_fc1 torch.Size([3072, 768])
    lora_te_text_model_encoder_layers_9_mlp_fc2 torch.Size([768, 3072])
    lora_te_text_model_encoder_layers_9_self_attn_k_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_9_self_attn_out_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_9_self_attn_q_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_9_self_attn_v_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_10_mlp_fc1 torch.Size([3072, 768])
    lora_te_text_model_encoder_layers_10_mlp_fc2 torch.Size([768, 3072])
    lora_te_text_model_encoder_layers_10_self_attn_k_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_10_self_attn_out_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_10_self_attn_q_proj torch.Size([768, 768])
    lora_te_text_model_encoder_layers_10_self_attn_v_proj torch.Size([768, 768])



    lora_unet_time_embedding_linear_1 torch.Size([1280, 320])
    lora_unet_time_embedding_linear_2 torch.Size([1280, 1280])



lora_unet_conv_in torch.Size([320, 4, 3, 3])

lora_unet_down_blocks_0_resnets_0_conv1 torch.Size([320, 320, 3, 3])
    lora_unet_down_blocks_0_resnets_0_time_emb_proj torch.Size([320, 1280])
lora_unet_down_blocks_0_resnets_0_conv2 torch.Size([320, 320, 3, 3])

lora_unet_down_blocks_0_attentions_0_proj_in torch.Size([320, 320, 1, 1])
    lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_k torch.Size([320, 320])
    lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_out_0 torch.Size([320, 320])
    lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_q torch.Size([320, 320])
    lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn1_to_v torch.Size([320, 320])
    lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_k torch.Size([320, 768])
    lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_out_0 torch.Size([320, 320])
    lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_q torch.Size([320, 320])
    lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_attn2_to_v torch.Size([320, 768])
    lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj torch.Size([2560, 320])
    lora_unet_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2 torch.Size([320, 1280])
lora_unet_down_blocks_0_attentions_0_proj_out torch.Size([320, 320, 1, 1])

lora_unet_down_blocks_0_resnets_1_conv1 torch.Size([320, 320, 3, 3])
    lora_unet_down_blocks_0_resnets_1_time_emb_proj torch.Size([320, 1280])
lora_unet_down_blocks_0_resnets_1_conv2 torch.Size([320, 320, 3, 3])

lora_unet_down_blocks_0_attentions_1_proj_in torch.Size([320, 320, 1, 1])
    lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_k torch.Size([320, 320])
    lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_out_0 torch.Size([320, 320])
    lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_q torch.Size([320, 320])
    lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn1_to_v torch.Size([320, 320])
    lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_k torch.Size([320, 768])
    lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_out_0 torch.Size([320, 320])
    lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_q torch.Size([320, 320])
    lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_attn2_to_v torch.Size([320, 768])
    lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj torch.Size([2560, 320])
    lora_unet_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2 torch.Size([320, 1280])
lora_unet_down_blocks_0_attentions_1_proj_out torch.Size([320, 320, 1, 1])

lora_unet_down_blocks_0_downsamplers_0_conv torch.Size([320, 320, 3, 3])

lora_unet_down_blocks_1_resnets_0_conv1 torch.Size([640, 320, 3, 3])
    lora_unet_down_blocks_1_resnets_0_time_emb_proj torch.Size([640, 1280])
lora_unet_down_blocks_1_resnets_0_conv2 torch.Size([640, 640, 3, 3])
lora_unet_down_blocks_1_resnets_0_conv_shortcut torch.Size([640, 320, 1, 1])

lora_unet_down_blocks_1_attentions_0_proj_in torch.Size([640, 640, 1, 1])
    lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k torch.Size([640, 640])
    lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0 torch.Size([640, 640])
    lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q torch.Size([640, 640])
    lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v torch.Size([640, 640])
    lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k torch.Size([640, 768])
    lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0 torch.Size([640, 640])
    lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q torch.Size([640, 640])
    lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v torch.Size([640, 768])
    lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj torch.Size([5120, 640])
    lora_unet_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2 torch.Size([640, 2560])
lora_unet_down_blocks_1_attentions_0_proj_out torch.Size([640, 640, 1, 1])

lora_unet_down_blocks_1_resnets_1_conv1 torch.Size([640, 640, 3, 3])
    lora_unet_down_blocks_1_resnets_1_time_emb_proj torch.Size([640, 1280])
lora_unet_down_blocks_1_resnets_1_conv2 torch.Size([640, 640, 3, 3])

lora_unet_down_blocks_1_attentions_1_proj_in torch.Size([640, 640, 1, 1])
    lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k torch.Size([640, 640])
    lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0 torch.Size([640, 640])
    lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q torch.Size([640, 640])
    lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v torch.Size([640, 640])
    lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k torch.Size([640, 768])
    lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0 torch.Size([640, 640])
    lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q torch.Size([640, 640])
    lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v torch.Size([640, 768])
    lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj torch.Size([5120, 640])
    lora_unet_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2 torch.Size([640, 2560])
lora_unet_down_blocks_1_attentions_1_proj_out torch.Size([640, 640, 1, 1])

lora_unet_down_blocks_1_downsamplers_0_conv torch.Size([640, 640, 3, 3])

lora_unet_down_blocks_2_resnets_0_conv1 torch.Size([1280, 640, 3, 3])
    lora_unet_down_blocks_2_resnets_0_time_emb_proj torch.Size([1280, 1280])
lora_unet_down_blocks_2_resnets_0_conv2 torch.Size([1280, 1280, 3, 3])
lora_unet_down_blocks_2_resnets_0_conv_shortcut torch.Size([1280, 640, 1, 1])
         
lora_unet_down_blocks_2_attentions_0_proj_in torch.Size([1280, 1280, 1, 1])
    lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k torch.Size([1280, 1280])
    lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0 torch.Size([1280, 1280])
    lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q torch.Size([1280, 1280])
    lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v torch.Size([1280, 1280])
    lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k torch.Size([1280, 768])
    lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0 torch.Size([1280, 1280])
    lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q torch.Size([1280, 1280])
    lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v torch.Size([1280, 768])
    lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj torch.Size([10240, 1280])
    lora_unet_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2 torch.Size([1280, 5120])
lora_unet_down_blocks_2_attentions_0_proj_out torch.Size([1280, 1280, 1, 1])

lora_unet_down_blocks_2_resnets_1_conv1 torch.Size([1280, 1280, 3, 3])
    lora_unet_down_blocks_2_resnets_1_time_emb_proj torch.Size([1280, 1280])
lora_unet_down_blocks_2_resnets_1_conv2 torch.Size([1280, 1280, 3, 3])     

lora_unet_down_blocks_2_attentions_1_proj_in torch.Size([1280, 1280, 1, 1])
    lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k torch.Size([1280, 1280])
    lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0 torch.Size([1280, 1280])
    lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q torch.Size([1280, 1280])
    lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v torch.Size([1280, 1280])
    lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k torch.Size([1280, 768])
    lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0 torch.Size([1280, 1280])
    lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q torch.Size([1280, 1280])
    lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v torch.Size([1280, 768])
    lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj torch.Size([10240, 1280])
    lora_unet_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2 torch.Size([1280, 5120])
lora_unet_down_blocks_2_attentions_1_proj_out torch.Size([1280, 1280, 1, 1])

lora_unet_down_blocks_2_downsamplers_0_conv torch.Size([1280, 1280, 3, 3])

lora_unet_down_blocks_3_resnets_0_conv1 torch.Size([1280, 1280, 3, 3])
    lora_unet_down_blocks_3_resnets_0_time_emb_proj torch.Size([1280, 1280])
lora_unet_down_blocks_3_resnets_0_conv2 torch.Size([1280, 1280, 3, 3])

lora_unet_down_blocks_3_resnets_1_conv1 torch.Size([1280, 1280, 3, 3])
    lora_unet_down_blocks_3_resnets_1_time_emb_proj torch.Size([1280, 1280])
lora_unet_down_blocks_3_resnets_1_conv2 torch.Size([1280, 1280, 3, 3])

lora_unet_mid_block_resnets_0_conv1 torch.Size([1280, 1280, 3, 3])
    lora_unet_mid_block_resnets_0_time_emb_proj torch.Size([1280, 1280])
lora_unet_mid_block_resnets_0_conv2 torch.Size([1280, 1280, 3, 3])

lora_unet_mid_block_attentions_0_proj_in torch.Size([1280, 1280, 1, 1])
    lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_k torch.Size([1280, 1280])
    lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_out_0 torch.Size([1280, 1280])
    lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_q torch.Size([1280, 1280])
    lora_unet_mid_block_attentions_0_transformer_blocks_0_attn1_to_v torch.Size([1280, 1280])
    lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_k torch.Size([1280, 768])
    lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_out_0 torch.Size([1280, 1280])
    lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_q torch.Size([1280, 1280])
    lora_unet_mid_block_attentions_0_transformer_blocks_0_attn2_to_v torch.Size([1280, 768])
    lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj torch.Size([10240, 1280])
    lora_unet_mid_block_attentions_0_transformer_blocks_0_ff_net_2 torch.Size([1280, 5120])
lora_unet_mid_block_attentions_0_proj_out torch.Size([1280, 1280, 1, 1])

lora_unet_mid_block_resnets_1_conv1 torch.Size([1280, 1280, 3, 3])
    lora_unet_mid_block_resnets_1_time_emb_proj torch.Size([1280, 1280])
lora_unet_mid_block_resnets_1_conv2 torch.Size([1280, 1280, 3, 3])

lora_unet_up_blocks_0_resnets_0_conv1 torch.Size([1280, 2560, 3, 3])
    lora_unet_up_blocks_0_resnets_0_time_emb_proj torch.Size([1280, 1280])
lora_unet_up_blocks_0_resnets_0_conv2 torch.Size([1280, 1280, 3, 3])
lora_unet_up_blocks_0_resnets_0_conv_shortcut torch.Size([1280, 2560, 1, 1])

lora_unet_up_blocks_0_resnets_1_conv1 torch.Size([1280, 2560, 3, 3])
    lora_unet_up_blocks_0_resnets_1_time_emb_proj torch.Size([1280, 1280])
lora_unet_up_blocks_0_resnets_1_conv2 torch.Size([1280, 1280, 3, 3])
lora_unet_up_blocks_0_resnets_1_conv_shortcut torch.Size([1280, 2560, 1, 1])

lora_unet_up_blocks_0_resnets_2_conv1 torch.Size([1280, 2560, 3, 3])
    lora_unet_up_blocks_0_resnets_2_time_emb_proj torch.Size([1280, 1280])
lora_unet_up_blocks_0_resnets_2_conv2 torch.Size([1280, 1280, 3, 3])
lora_unet_up_blocks_0_resnets_2_conv_shortcut torch.Size([1280, 2560, 1, 1])

lora_unet_up_blocks_0_upsamplers_0_conv torch.Size([1280, 1280, 3, 3])

lora_unet_up_blocks_1_resnets_0_conv1 torch.Size([1280, 2560, 3, 3])
    lora_unet_up_blocks_1_resnets_0_time_emb_proj torch.Size([1280, 1280])
lora_unet_up_blocks_1_resnets_0_conv2 torch.Size([1280, 1280, 3, 3])
lora_unet_up_blocks_1_resnets_0_conv_shortcut torch.Size([1280, 2560, 1, 1])

lora_unet_up_blocks_1_attentions_0_proj_in torch.Size([1280, 1280, 1, 1])
    lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_k torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_out_0 torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_q torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn1_to_v torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_k torch.Size([1280, 768])
    lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_out_0 torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_q torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_attn2_to_v torch.Size([1280, 768])
    lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj torch.Size([10240, 1280])
    lora_unet_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2 torch.Size([1280, 5120])
lora_unet_up_blocks_1_attentions_0_proj_out torch.Size([1280, 1280, 1, 1])

lora_unet_up_blocks_1_resnets_1_conv1 torch.Size([1280, 2560, 3, 3])
    lora_unet_up_blocks_1_resnets_1_time_emb_proj torch.Size([1280, 1280])
lora_unet_up_blocks_1_resnets_1_conv2 torch.Size([1280, 1280, 3, 3])
lora_unet_up_blocks_1_resnets_1_conv_shortcut torch.Size([1280, 2560, 1, 1])

lora_unet_up_blocks_1_attentions_1_proj_in torch.Size([1280, 1280, 1, 1])
    lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_k torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_out_0 torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_q torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn1_to_v torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_k torch.Size([1280, 768])
    lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_out_0 torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_q torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_attn2_to_v torch.Size([1280, 768])
    lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj torch.Size([10240, 1280])
    lora_unet_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2 torch.Size([1280, 5120])
lora_unet_up_blocks_1_attentions_1_proj_out torch.Size([1280, 1280, 1, 1])

lora_unet_up_blocks_1_resnets_2_conv1 torch.Size([1280, 1920, 3, 3])
    lora_unet_up_blocks_1_resnets_2_time_emb_proj torch.Size([1280, 1280])
lora_unet_up_blocks_1_resnets_2_conv2 torch.Size([1280, 1280, 3, 3])
lora_unet_up_blocks_1_resnets_2_conv_shortcut torch.Size([1280, 1920, 1, 1])

lora_unet_up_blocks_1_attentions_2_proj_in torch.Size([1280, 1280, 1, 1])
    lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_k torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_out_0 torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_q torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn1_to_v torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_k torch.Size([1280, 768])
    lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_out_0 torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_q torch.Size([1280, 1280])
    lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_attn2_to_v torch.Size([1280, 768])
    lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj torch.Size([10240, 1280])
    lora_unet_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2 torch.Size([1280, 5120])
lora_unet_up_blocks_1_attentions_2_proj_out torch.Size([1280, 1280, 1, 1])

lora_unet_up_blocks_1_upsamplers_0_conv torch.Size([1280, 1280, 3, 3])

lora_unet_up_blocks_2_resnets_0_conv1 torch.Size([640, 1920, 3, 3])
    lora_unet_up_blocks_2_resnets_0_time_emb_proj torch.Size([640, 1280])
lora_unet_up_blocks_2_resnets_0_conv2 torch.Size([640, 640, 3, 3])
lora_unet_up_blocks_2_resnets_0_conv_shortcut torch.Size([640, 1920, 1, 1])

lora_unet_up_blocks_2_attentions_0_proj_in torch.Size([640, 640, 1, 1])
    lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_k torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_out_0 torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_q torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn1_to_v torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_k torch.Size([640, 768])
    lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_out_0 torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_q torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_attn2_to_v torch.Size([640, 768])
    lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj torch.Size([5120, 640])
    lora_unet_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2 torch.Size([640, 2560])
lora_unet_up_blocks_2_attentions_0_proj_out torch.Size([640, 640, 1, 1])

lora_unet_up_blocks_2_resnets_1_conv1 torch.Size([640, 1280, 3, 3])
    lora_unet_up_blocks_2_resnets_1_time_emb_proj torch.Size([640, 1280])
lora_unet_up_blocks_2_resnets_1_conv2 torch.Size([640, 640, 3, 3])
lora_unet_up_blocks_2_resnets_1_conv_shortcut torch.Size([640, 1280, 1, 1])

lora_unet_up_blocks_2_attentions_1_proj_in torch.Size([640, 640, 1, 1])
    lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_k torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_out_0 torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_q torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn1_to_v torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_k torch.Size([640, 768])
    lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_out_0 torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_q torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_attn2_to_v torch.Size([640, 768])
    lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj torch.Size([5120, 640])
    lora_unet_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2 torch.Size([640, 2560])
lora_unet_up_blocks_2_attentions_1_proj_out torch.Size([640, 640, 1, 1])

lora_unet_up_blocks_2_resnets_2_conv1 torch.Size([640, 960, 3, 3])
    lora_unet_up_blocks_2_resnets_2_time_emb_proj torch.Size([640, 1280])
lora_unet_up_blocks_2_resnets_2_conv2 torch.Size([640, 640, 3, 3])
lora_unet_up_blocks_2_resnets_2_conv_shortcut torch.Size([640, 960, 1, 1])

lora_unet_up_blocks_2_attentions_2_proj_in torch.Size([640, 640, 1, 1])
    lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_k torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_out_0 torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_q torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn1_to_v torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_k torch.Size([640, 768])
    lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_out_0 torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_q torch.Size([640, 640])
    lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_attn2_to_v torch.Size([640, 768])
    lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj torch.Size([5120, 640])
    lora_unet_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2 torch.Size([640, 2560])
lora_unet_up_blocks_2_attentions_2_proj_out torch.Size([640, 640, 1, 1])

lora_unet_up_blocks_2_upsamplers_0_conv torch.Size([640, 640, 3, 3])

lora_unet_up_blocks_3_resnets_0_conv1 torch.Size([320, 960, 3, 3])
    lora_unet_up_blocks_3_resnets_0_time_emb_proj torch.Size([320, 1280])
lora_unet_up_blocks_3_resnets_0_conv2 torch.Size([320, 320, 3, 3])
lora_unet_up_blocks_3_resnets_0_conv_shortcut torch.Size([320, 960, 1, 1])

lora_unet_up_blocks_3_attentions_0_proj_in torch.Size([320, 320, 1, 1])
    lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_k torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_out_0 torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_q torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn1_to_v torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_k torch.Size([320, 768])
    lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_out_0 torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_q torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_attn2_to_v torch.Size([320, 768])
    lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj torch.Size([2560, 320])
    lora_unet_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2 torch.Size([320, 1280])
lora_unet_up_blocks_3_attentions_0_proj_out torch.Size([320, 320, 1, 1])

lora_unet_up_blocks_3_resnets_1_conv1 torch.Size([320, 640, 3, 3])
    lora_unet_up_blocks_3_resnets_1_time_emb_proj torch.Size([320, 1280])
lora_unet_up_blocks_3_resnets_1_conv2 torch.Size([320, 320, 3, 3])
lora_unet_up_blocks_3_resnets_1_conv_shortcut torch.Size([320, 640, 1, 1])

lora_unet_up_blocks_3_attentions_1_proj_in torch.Size([320, 320, 1, 1])
    lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_k torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_out_0 torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_q torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn1_to_v torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_k torch.Size([320, 768])
    lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_out_0 torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_q torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_attn2_to_v torch.Size([320, 768])
    lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj torch.Size([2560, 320])
    lora_unet_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2 torch.Size([320, 1280])
lora_unet_up_blocks_3_attentions_1_proj_out torch.Size([320, 320, 1, 1])

lora_unet_up_blocks_3_resnets_2_conv1 torch.Size([320, 640, 3, 3])
    lora_unet_up_blocks_3_resnets_2_time_emb_proj torch.Size([320, 1280])
lora_unet_up_blocks_3_resnets_2_conv2 torch.Size([320, 320, 3, 3])
lora_unet_up_blocks_3_resnets_2_conv_shortcut torch.Size([320, 640, 1, 1])

lora_unet_up_blocks_3_attentions_2_proj_in torch.Size([320, 320, 1, 1])
    lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_k torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_out_0 torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_q torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn1_to_v torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_k torch.Size([320, 768])
    lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_out_0 torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_q torch.Size([320, 320])
    lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_attn2_to_v torch.Size([320, 768])
    lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj torch.Size([2560, 320])
    lora_unet_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2 torch.Size([320, 1280])
lora_unet_up_blocks_3_attentions_2_proj_out torch.Size([320, 320, 1, 1])

lora_unet_conv_out torch.Size([4, 320, 3, 3])