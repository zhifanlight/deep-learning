<script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

# ResNet Variants

## ResNet with Stochastic Depth

### 背景

- 深网络的表达能力更强，但是存在以下问题：

	- 反向传播时，容易发生梯度消失

	- 前向计算时，由于卷积和叠乘，深层网络无法有效利用浅层信息

	- 计算资源大、训练时间长

### 思想

- 创建深网络，训练时随机跳过部分残差块，测试时使用完整的网络

	- 训练时，按概率 \\(P\_{l}\\) 保留第 \\(l\\) 个残差块的卷积分支

		- 当跳过第 \\(l\\) 个残差块的卷积分支时，由于恒等映射无需前向计算和反向传播，相当于跳过整个残差块

			- 由于进入残差块前经过 ReLU 函数，该残差块的输入均为正数；再次经过残差块后的 ReLU 函数时，结果不变

		- 由于深层网络要用到浅层特征，一般浅层残差块的保留率较高，深层残差块的保留率较低

		- 实际训练中，保留率按深度递减时效果较好：

			$$ P\_{0} = 1 \quad P\_{L} = 0.5 \quad P\_{l} = 1 - \frac{l}{L} (1 - P\_{L}) $$
			
			- 其中 \\(P\_{0}\\) 表示输入层，\\(P\_{1}\\) 表示第一个残差块，\\(P\_{L}\\) 表示最后一个残差块

			- 在该组超参数下，训练时的网络平均深度为 \\(\frac{3L}{4}\\)
	
	- 测试时，使用完整的网络

		- 为了训练与测试的一致性，需要将残差块的卷积分支输出缩放到原来的 \\(P\_{l}\\)

- 每个 batch 经过的网络结构不同，相当于不同模型的集成学习，进一步降低错误率

- 网络深度的减少，更利于前向计算时的浅层特征利用，以及反向传播时的梯度流动

### 结果

- 原始 ResNet 在深度达到 1000 时出现网络退化，而随机深度的 ResNet 不存在该问题

- 相比原始 ResNet，训练时间减少 \\(25\%\\)

## Wide ResNet

### 思想

- 对原始 ResNet 进行改进：减小深度，增加宽度

	- 使用只有几十层的浅网络

	- 增加卷积层提取的特征数

- 使用 Pre-Activation 的残差块结构，同一块内的卷积层之间使用 Dropout：

	$$ BN \rightarrow ReLU \rightarrow Conv \rightarrow Dropout \rightarrow BN \rightarrow ReLU \rightarrow Conv $$

### 结果

- 证明了 ResNet 中发挥主要作用的残差块，而不是网络深度

- 参数量相同时，宽网络能充分利用 GPU 并行性，训练速度比原始 ResNet 快

- 参数量相同时，随着宽度增加，残差块提取的特征越来越强，分类错误率越来越低