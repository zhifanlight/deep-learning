# $\mathrm{ResNeXt}$

## 背景

- 传统的模型要提高准确率，都是加深或加宽；但随着通道数、卷积核大小等超参数的增加，网络的设计难度和计算开销也会增加

- $\mathrm{ResNeXt}$ 的结构可以在同等参数量下提高准确度，同时也减少了超参数的数量

  - 残差块的子模块结构相同，减少了设计难度和超参数量

## 思想

- 同时采用 $\mathrm{VGG}$ 堆叠网络思想和 $\mathrm{Inception}$ “拆分-变换-合并”思想，构建如下残差块：

  <center>
  <img src="images/resnext.png"/>
  </center>

  - 图（$\mathrm{a}$）是基本的 $\mathrm{ResNeXt}$ 结构

    - 分别对输入的 $256$ 个特征图进行 $1 \times 1$ 卷积，得到 $32$ 组特征图，每组 $4$ 个

    - 对每一组特征图，进行 $3 \times 3$ 卷积

    - 对每一组特征图，进行 $1 \times 1$ 卷积，得到 $256$ 个特征图

    - 不同组的特征图按通道相加后，再与 $\mathrm{shortcut}$ 分支相加

  - 图（$\mathrm{b}$）是 图（$\mathrm{a}$）的等价形式

    - 每组特征图先做卷积再相加，相当于先拼接特征图再统一做卷积

  - 图（$\mathrm{c}$）是图（$\mathrm{b}$）的等价形式

    - 对所有输入通道分别做卷积再拼接，相当于对输入通道做分组卷积

  - 由于图（$\mathrm{c}$）实现方便，在工程中采用图（$\mathrm{c}$）的形式

    - 对输入的 $256$ 个特征图进行 $1 \times 1$ 卷积，得到 $128$ 个特征图

    - 进行 $\mathrm{group} = 32$ 的 $3 \times 3$ 分组卷积，得到 $128$ 个特征图

    - 对 $128$ 个特征图进行 $1 \times 1$ 卷积，得到 $256$ 个特征图

    - 与 $\mathrm{shotcut}$ 分支相加后作为整个残差块的输出

- 整个网络设计原则如下：

  - 残差块的输出特征图维度相同时，使用相同的超参数（通道数、卷积核大小等）

  - 特征图维度减半时，通道数翻倍，以保证所有残差块具有相似的计算复杂度

- 残差块的分组数又称为 $\mathrm{Cardinality}$

## 结果

- 参数量相同时，$\mathrm{ResNeXt}$ 比 $\mathrm{ResNet}$ 结果更好

  - $\mathrm{ResNeXt}$ 第一个 $1 \times 1$ 卷积通道数是 $\mathrm{ResNet}$ 的两倍，可以提取更充分的特征

  - 由于 $\mathrm{ResNeXt}$ 使用了分组卷积，参数量和计算量反而比 $\mathrm{ResNet}$ 少

- 增加 $\mathrm{Cardinality}$ 比增加模型的深度或宽度效果更好

- $\mathrm{ResNeXt}$ 网络结构更简单，仅需要少量的超参数来描述